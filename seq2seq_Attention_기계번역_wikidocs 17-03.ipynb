{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4837da0e-6034-449d-8973-92fd28c365ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e39988-013d-46ba-b431-e6f14ded79a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c119d1-a661-47d9-b1bd-cbf871c62036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    # 악센트가 있는 언어의 경우에 필수적이지만 그 외에는 필수적이지 않다.\n",
    "    # 프랑스어 악센트(accent) 삭제\n",
    "    # 예시 : 'déjà diné' -> deja dine\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79367d0e-cb26-4f1d-a35c-a66a6c3818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sent):\n",
    "    # 악센트 삭제 함수 호출\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    # 다수 개의 공백을 하나의 공백으로 치환\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ece3841-102f-44e2-96fb-25312cb8549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "    \n",
    "    with open(\"fra.txt\", \"r\", encoding=\"utf-8\") as lines: # 기본 인코딩이 CP949(한국어 윈도우 기본 인코딩)로 설정되어 있어서 문제가 발생\n",
    "        for i, line in enumerate(lines):\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "            \n",
    "            # source 데이터 전처리\n",
    "            src_line = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line)\n",
    "            decoder_input.append(tar_line_in)\n",
    "            decoder_target.append(tar_line_out)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "                \n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11efbfe-58ef-4886-8879-96358d2a9c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 영어 문장 : Have you had dinner?\n",
      "전처리 후 영어 문장 : have you had dinner ?\n",
      "전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n",
      "전처리 후 프랑스어 문장 : avez vous deja dine ?\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "\n",
    "print('전처리 전 영어 문장 :', en_sent)\n",
    "print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n",
    "print('전처리 전 프랑스어 문장 :', fr_sent)\n",
    "print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbeda4f4-bbb8-4bc8-9f78-63744eeec730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n",
      "디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n",
      "디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print('인코더의 입력 :',sents_en_in[:5])\n",
    "print('디코더의 입력 :',sents_fra_in[:5])\n",
    "print('디코더의 레이블 :',sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c9a8a2-7702-4699-be09-e5ea38d71d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sents):\n",
    "    word_list = []\n",
    "\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word_list.append(word)\n",
    "\n",
    "    # 각 단어별 등장 빈도를 계산하여 등장 빈도가 높은 순서로 정렬\n",
    "    word_counts = Counter(word_list)\n",
    "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    word_to_index = {}\n",
    "    word_to_index['<PAD>'] = 0\n",
    "    word_to_index['<UNK>'] = 1\n",
    "\n",
    "    # 등장 빈도가 높은 단어일수록 낮은 정수를 부여\n",
    "    for index, word in enumerate(vocab) :\n",
    "        word_to_index[word] = index + 2\n",
    "\n",
    "    return word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7287d0-d647-48eb-8a7c-9bed932318db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4486, 프랑스어 단어 집합의 크기 : 7879\n"
     ]
    }
   ],
   "source": [
    "src_vocab = build_vocab(sents_en_in)\n",
    "tar_vocab = build_vocab(sents_fra_in + sents_fra_out)\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tar_vocab_size = len(tar_vocab)\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36482258-d209-4ee3-8612-7db28ddda76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = {v: k for k, v in src_vocab.items()}\n",
    "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
    "\n",
    "def texts_to_sequences(sents, word_to_index):\n",
    "    encoded_X_data = []\n",
    "    for sent in tqdm(sents):\n",
    "        index_sequences = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                index_sequences.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                index_sequences.append(word_to_index['<UNK>'])\n",
    "        encoded_X_data.append(index_sequences)\n",
    "    return encoded_X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6744424d-8fe9-4526-9e9c-dfbe38b2eb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 33000/33000 [00:00<00:00, 173059.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33000/33000 [00:00<00:00, 412263.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 33000/33000 [00:00<00:00, 364874.00it/s]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = texts_to_sequences(sents_en_in, src_vocab)\n",
    "decoder_input = texts_to_sequences(sents_fra_in, tar_vocab)\n",
    "decoder_target = texts_to_sequences(sents_fra_out, tar_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b8eb0c-6b08-4670-97c3-f3535cb1e000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
      "Index: 1, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
      "Index: 2, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
      "Index: 3, 정수 인코딩 전: ['go', '.'], 정수 인코딩 후: [27, 2]\n",
      "Index: 4, 정수 인코딩 전: ['hi', '.'], 정수 인코딩 후: [736, 2]\n"
     ]
    }
   ],
   "source": [
    "# 상위 5개의 샘플에 대해서 정수 인코딩 전, 후 문장 출력\n",
    "# 인코더 입력이므로 <sos>나 <eos>가 없음\n",
    "for i, (item1, item2) in zip(range(5), zip(sents_en_in, encoder_input)):\n",
    "    print(f\"Index: {i}, 정수 인코딩 전: {item1}, 정수 인코딩 후: {item2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48dd658d-5a52-4799-9427-e94837657973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sentences, max_len=None):\n",
    "    # 최대 길이 값이 주어지지 않을 경우 데이터 내 최대 길이로 패딩\n",
    "    if max_len is None:\n",
    "        max_len = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d221e80c-3cd3-40fe-aef8-5c6b32e2612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input)\n",
    "decoder_input = pad_sequences(decoder_input)\n",
    "decoder_target = pad_sequences(decoder_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7013e82e-b285-4269-871c-f7354399953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더의 입력의 크기(shape) : (33000, 7)\n",
      "디코더의 입력의 크기(shape) : (33000, 16)\n",
      "디코더의 레이블의 크기(shape) : (33000, 16)\n"
     ]
    }
   ],
   "source": [
    "print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n",
    "print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n",
    "print('디코더의 레이블의 크기(shape) :',decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "594b280a-e500-4792-9e9e-be95a90501c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시퀀스 : [16383 29263 32603 ...  7460 29764 21958]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print('랜덤 시퀀스 :',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b683a59f-9fd6-4703-850b-441660e46deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b924a9-ec1b-471a-9f72-201376c6fe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'good', 'are', 'you', '?', '<PAD>', '<PAD>']\n",
      "['<sos>', 'jusqu', 'a', 'quel', 'point', 'etes', 'vous', 'bonnes', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['jusqu', 'a', 'quel', 'point', 'etes', 'vous', 'bonnes', '?', '<eos>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print([index_to_src[word] for word in encoder_input[30997]])\n",
    "print([index_to_tar[word] for word in decoder_input[30997]])\n",
    "print([index_to_tar[word] for word in decoder_target[30997]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b4a2c78-b2dc-4dc6-86bc-c965160deb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터의 개수 : 3300\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(33000*0.1)\n",
    "print('검증 데이터의 개수 :',n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "883681ad-d3b5-4068-bd51-baebc912ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fce868fb-b84f-47a5-9b85-a34604687e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 source 데이터의 크기 : (29700, 7)\n",
      "훈련 target 데이터의 크기 : (29700, 16)\n",
      "훈련 target 레이블의 크기 : (29700, 16)\n",
      "테스트 source 데이터의 크기 : (3300, 7)\n",
      "테스트 target 데이터의 크기 : (3300, 16)\n",
      "테스트 target 레이블의 크기 : (3300, 16)\n"
     ]
    }
   ],
   "source": [
    "print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n",
    "print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n",
    "print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n",
    "print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n",
    "print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n",
    "print('테스트 target 레이블의 크기 :',decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dacc24d-d39e-4f13-9ee1-52030e2203d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "embedding_dim = 256\n",
    "hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70a08f82-5a40-4e5e-adf7-93e58c0d2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embedding_dim, hidden_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_units, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape == (batch_size, seq_len, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # hidden.shape == (1, batch_size, hidden_units), cell.shape == (1, batch_size, hidden_units)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7299452b-9a37-4cfe-8b72-2b7903ca6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, hidden_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(tar_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_units, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, tar_vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, hidden, cell):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Dot product attention\n",
    "        # attention_scores.shape: (batch_size, source_seq_len, 1)\n",
    "        attention_scores = torch.bmm(encoder_outputs, hidden.transpose(0, 1).transpose(1, 2))\n",
    "\n",
    "        # attention_weights.shape: (batch_size, source_seq_len, 1)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "\n",
    "        # context_vector.shape: (batch_size, 1, hidden_units)\n",
    "        context_vector = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs)\n",
    "\n",
    "        # Repeat context_vector to match seq_len\n",
    "        # context_vector_repeated.shape: (batch_size, target_seq_len, hidden_units)\n",
    "        seq_len = x.shape[1]\n",
    "        context_vector_repeated = context_vector.repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate context vector and embedded input\n",
    "        # x.shape: (batch_size, target_seq_len, embedding_dim + hidden_units)\n",
    "        x = torch.cat((x, context_vector_repeated), dim=2)\n",
    "\n",
    "        # output.shape: (batch_size, target_seq_len, hidden_units)\n",
    "        # hidden.shape: (1, batch_size, hidden_units)\n",
    "        # cell.shape: (1, batch_size, hidden_units)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "\n",
    "        # output.shape: (batch_size, target_seq_len, tar_vocab_size)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cdf4bcb-87fb-42b8-b12c-6777865af7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        output, _, _ = self.decoder(trg, encoder_outputs, hidden, cell)\n",
    "        return output\n",
    "\n",
    "encoder = Encoder(src_vocab_size, embedding_dim, hidden_units)\n",
    "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_units)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9435009-a665-43c5-b32c-83adf025caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "            # 순방향 전파\n",
    "            # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
    "            outputs = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "            # 손실 계산\n",
    "            # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
    "            # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산 (패딩 토큰 제외)\n",
    "            mask = decoder_targets != 0\n",
    "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f516e95-a9e2-4f40-8a13-907619e556c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4486, 256, padding_idx=0)\n",
       "    (lstm): LSTM(256, 256, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7879, 256, padding_idx=0)\n",
       "    (lstm): LSTM(512, 256, batch_first=True)\n",
       "    (fc): Linear(in_features=256, out_features=7879, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n",
    "decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n",
    "decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n",
    "\n",
    "encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n",
    "decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n",
    "decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 학습 설정\n",
    "num_epochs = 30\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a0eba2c-042c-4820-a2b0-c8cfc7ddf30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 | Train Loss: 2.8638 | Train Acc: 0.5404 | Valid Loss: 2.9843 | Valid Acc: 0.5367\n",
      "Validation loss improved from inf to 2.9843. 체크포인트를 저장합니다.\n",
      "Epoch: 2/30 | Train Loss: 2.2042 | Train Acc: 0.6119 | Valid Loss: 2.4331 | Valid Acc: 0.6000\n",
      "Validation loss improved from 2.9843 to 2.4331. 체크포인트를 저장합니다.\n",
      "Epoch: 3/30 | Train Loss: 1.7884 | Train Acc: 0.6539 | Valid Loss: 2.1351 | Valid Acc: 0.6293\n",
      "Validation loss improved from 2.4331 to 2.1351. 체크포인트를 저장합니다.\n",
      "Epoch: 4/30 | Train Loss: 1.4603 | Train Acc: 0.7014 | Valid Loss: 1.9294 | Valid Acc: 0.6580\n",
      "Validation loss improved from 2.1351 to 1.9294. 체크포인트를 저장합니다.\n",
      "Epoch: 5/30 | Train Loss: 1.1948 | Train Acc: 0.7411 | Valid Loss: 1.7736 | Valid Acc: 0.6765\n",
      "Validation loss improved from 1.9294 to 1.7736. 체크포인트를 저장합니다.\n",
      "Epoch: 6/30 | Train Loss: 0.9749 | Train Acc: 0.7799 | Valid Loss: 1.6641 | Valid Acc: 0.6901\n",
      "Validation loss improved from 1.7736 to 1.6641. 체크포인트를 저장합니다.\n",
      "Epoch: 7/30 | Train Loss: 0.7993 | Train Acc: 0.8170 | Valid Loss: 1.5861 | Valid Acc: 0.7010\n",
      "Validation loss improved from 1.6641 to 1.5861. 체크포인트를 저장합니다.\n",
      "Epoch: 8/30 | Train Loss: 0.6529 | Train Acc: 0.8467 | Valid Loss: 1.5249 | Valid Acc: 0.7115\n",
      "Validation loss improved from 1.5861 to 1.5249. 체크포인트를 저장합니다.\n",
      "Epoch: 9/30 | Train Loss: 0.5344 | Train Acc: 0.8706 | Valid Loss: 1.4775 | Valid Acc: 0.7171\n",
      "Validation loss improved from 1.5249 to 1.4775. 체크포인트를 저장합니다.\n",
      "Epoch: 10/30 | Train Loss: 0.4490 | Train Acc: 0.8869 | Valid Loss: 1.4586 | Valid Acc: 0.7192\n",
      "Validation loss improved from 1.4775 to 1.4586. 체크포인트를 저장합니다.\n",
      "Epoch: 11/30 | Train Loss: 0.3809 | Train Acc: 0.9008 | Valid Loss: 1.4461 | Valid Acc: 0.7266\n",
      "Validation loss improved from 1.4586 to 1.4461. 체크포인트를 저장합니다.\n",
      "Epoch: 12/30 | Train Loss: 0.3301 | Train Acc: 0.9093 | Valid Loss: 1.4376 | Valid Acc: 0.7262\n",
      "Validation loss improved from 1.4461 to 1.4376. 체크포인트를 저장합니다.\n",
      "Epoch: 13/30 | Train Loss: 0.2901 | Train Acc: 0.9152 | Valid Loss: 1.4408 | Valid Acc: 0.7309\n",
      "Epoch: 14/30 | Train Loss: 0.2618 | Train Acc: 0.9205 | Valid Loss: 1.4484 | Valid Acc: 0.7282\n",
      "Epoch: 15/30 | Train Loss: 0.2410 | Train Acc: 0.9225 | Valid Loss: 1.4607 | Valid Acc: 0.7265\n",
      "Epoch: 16/30 | Train Loss: 0.2246 | Train Acc: 0.9245 | Valid Loss: 1.4689 | Valid Acc: 0.7256\n",
      "Epoch: 17/30 | Train Loss: 0.2098 | Train Acc: 0.9269 | Valid Loss: 1.4866 | Valid Acc: 0.7294\n",
      "Epoch: 18/30 | Train Loss: 0.2001 | Train Acc: 0.9279 | Valid Loss: 1.4884 | Valid Acc: 0.7279\n",
      "Epoch: 19/30 | Train Loss: 0.1900 | Train Acc: 0.9288 | Valid Loss: 1.5087 | Valid Acc: 0.7276\n",
      "Epoch: 20/30 | Train Loss: 0.1863 | Train Acc: 0.9300 | Valid Loss: 1.5065 | Valid Acc: 0.7298\n",
      "Epoch: 21/30 | Train Loss: 0.1815 | Train Acc: 0.9303 | Valid Loss: 1.5274 | Valid Acc: 0.7278\n",
      "Epoch: 22/30 | Train Loss: 0.1743 | Train Acc: 0.9308 | Valid Loss: 1.5407 | Valid Acc: 0.7296\n",
      "Epoch: 23/30 | Train Loss: 0.1692 | Train Acc: 0.9310 | Valid Loss: 1.5380 | Valid Acc: 0.7292\n",
      "Epoch: 24/30 | Train Loss: 0.1662 | Train Acc: 0.9317 | Valid Loss: 1.5485 | Valid Acc: 0.7264\n",
      "Epoch: 25/30 | Train Loss: 0.1638 | Train Acc: 0.9314 | Valid Loss: 1.5620 | Valid Acc: 0.7311\n",
      "Epoch: 26/30 | Train Loss: 0.1611 | Train Acc: 0.9317 | Valid Loss: 1.5581 | Valid Acc: 0.7280\n",
      "Epoch: 27/30 | Train Loss: 0.1598 | Train Acc: 0.9323 | Valid Loss: 1.5736 | Valid Acc: 0.7297\n",
      "Epoch: 28/30 | Train Loss: 0.1561 | Train Acc: 0.9322 | Valid Loss: 1.5901 | Valid Acc: 0.7277\n",
      "Epoch: 29/30 | Train Loss: 0.1561 | Train Acc: 0.9316 | Valid Loss: 1.5866 | Valid Acc: 0.7271\n",
      "Epoch: 30/30 | Train Loss: 0.1561 | Train Acc: 0.9324 | Valid Loss: 1.6063 | Valid Acc: 0.7271\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 훈련 모드\n",
    "    model.train()\n",
    "\n",
    "    for encoder_inputs, decoder_inputs, decoder_targets in train_dataloader:\n",
    "        encoder_inputs = encoder_inputs.to(device)\n",
    "        decoder_inputs = decoder_inputs.to(device)\n",
    "        decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "        # 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순방향 전파\n",
    "        # outputs.shape == (batch_size, seq_len, tar_vocab_size)\n",
    "        outputs = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "        # 손실 계산 및 역방향 전파\n",
    "        # outputs.view(-1, outputs.size(-1))의 shape는 (batch_size * seq_len, tar_vocab_size)\n",
    "        # decoder_targets.view(-1)의 shape는 (batch_size * seq_len)\n",
    "        loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_acc = evaluation(model, train_dataloader, loss_function, device)\n",
    "    valid_loss, valid_acc = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if valid_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {valid_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_seq2seq_attention_model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b0fc548-055a-4626-9ca5-3a55d5dcf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model validation loss: 1.4376\n",
      "Best model validation accuracy: 0.7262\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model.load_state_dict(torch.load('best_seq2seq_attention_model_checkpoint.pth'))\n",
    "\n",
    "# 모델을 device에 올립니다.\n",
    "model.to(device)\n",
    "\n",
    "# 검증 데이터에 대한 정확도와 손실 계산\n",
    "val_loss, val_accuracy = evaluation(model, valid_dataloader, loss_function, device)\n",
    "\n",
    "print(f'Best model validation loss: {val_loss:.4f}')\n",
    "print(f'Best model validation accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e3bb080-11b0-41d5-8717-5d1a3e3db299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(tar_vocab['<sos>'])\n",
    "print(tar_vocab['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88844559-0bea-49e9-a524-77f490964502",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = {v: k for k, v in src_vocab.items()}\n",
    "index_to_tar = {v: k for k, v in tar_vocab.items()}\n",
    "\n",
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_src(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0):\n",
    "      sentence = sentence + index_to_src[encoded_word] + ' '\n",
    "  return sentence\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq_to_tar(input_seq):\n",
    "  sentence = ''\n",
    "  for encoded_word in input_seq:\n",
    "    if(encoded_word != 0 and encoded_word != tar_vocab['<sos>'] and encoded_word != tar_vocab['<eos>']):\n",
    "      sentence = sentence + index_to_tar[encoded_word] + ' '\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f40422f-1d6c-4d02-ac84-f312913bd3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[353   7  45  24   0   0   0]\n",
      "[  3 179  11   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[179  11   4   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_test[25])\n",
    "print(decoder_input_test[25])\n",
    "print(decoder_target_test[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d85c81e-b1fe-471c-a002-36eab2d9dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, max_output_len, int_to_src_token, int_to_tar_token):\n",
    "    encoder_inputs = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # 인코더의 초기 상태 설정\n",
    "    encoder_outputs, hidden, cell = model.encoder(encoder_inputs)\n",
    "\n",
    "    # 시작 토큰 <sos>을 디코더의 첫 입력으로 설정\n",
    "    # unsqueeze(0)는 배치 차원을 추가하기 위함.\n",
    "    decoder_input = torch.tensor([3], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    decoded_tokens = []\n",
    "\n",
    "    # for문을 도는 것 == 디코더의 각 시점\n",
    "    for _ in range(max_output_len):\n",
    "        output, hidden, cell = model.decoder(decoder_input, encoder_outputs, hidden, cell)\n",
    "\n",
    "        # 소프트맥스 회귀를 수행. 예측 단어의 인덱스\n",
    "        output_token = output.argmax(dim=-1).item()\n",
    "\n",
    "        # 종료 토큰 <eos>\n",
    "        if output_token == 4:\n",
    "            break\n",
    "\n",
    "        # 각 시점의 단어(정수)는 decoded_tokens에 누적하였다가 최종 번역 시퀀스로 리턴합니다.\n",
    "        decoded_tokens.append(output_token)\n",
    "\n",
    "        # 현재 시점의 예측. 다음 시점의 입력으로 사용된다.\n",
    "        decoder_input = torch.tensor([output_token], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    return ' '.join(int_to_tar_token[token] for token in decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab3e40d7-6567-4210-8f9a-01bbf1f8d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : feed the hamster . \n",
      "정답문장 : nourrissez le hamster . \n",
      "번역문장 : nourrissez le hamster .\n",
      "--------------------------------------------------\n",
      "입력문장 : let s hope not . \n",
      "정답문장 : esperons que non . \n",
      "번역문장 : esperons ne pas .\n",
      "--------------------------------------------------\n",
      "입력문장 : you re clever . \n",
      "정답문장 : vous etes malin . \n",
      "번역문장 : vous etes malin .\n",
      "--------------------------------------------------\n",
      "입력문장 : beat it . \n",
      "정답문장 : disparais ! \n",
      "번역문장 : fiche le .\n",
      "--------------------------------------------------\n",
      "입력문장 : it s all i ve got . \n",
      "정답문장 : c est tout ce que j ai . \n",
      "번역문장 : c est tout ce que j ai .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_train[seq_index]\n",
    "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_train[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_train[seq_index]))\n",
    "  print(\"번역문장 :\",translated_text)\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c38b556b-989e-41fb-b29d-5f2e3d279983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력문장 : take mine . \n",
      "정답문장 : prenez les miennes . \n",
      "번역문장 : prenez les miennes .\n",
      "--------------------------------------------------\n",
      "입력문장 : are they in love ? \n",
      "정답문장 : sont elles amoureuses ? \n",
      "번역문장 : sont ils amoureux ?\n",
      "--------------------------------------------------\n",
      "입력문장 : just say yes . \n",
      "정답문장 : dis juste oui ! \n",
      "번역문장 : dites juste juste oui .\n",
      "--------------------------------------------------\n",
      "입력문장 : i pushed tom away . \n",
      "정답문장 : je repoussai tom . \n",
      "번역문장 : je fais un calin tom .\n",
      "--------------------------------------------------\n",
      "입력문장 : they approve . \n",
      "정답문장 : elles approuvent . \n",
      "번역문장 : ils approuvent .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "  input_seq = encoder_input_test[seq_index]\n",
    "  translated_text = decode_sequence(input_seq, model, src_vocab_size, tar_vocab_size, 20, index_to_src, index_to_tar)\n",
    "\n",
    "  print(\"입력문장 :\",seq_to_src(encoder_input_test[seq_index]))\n",
    "  print(\"정답문장 :\",seq_to_tar(decoder_input_test[seq_index]))\n",
    "  print(\"번역문장 :\",translated_text)\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be86a5d9-f38f-47fd-b266-510d7599442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train BLEU: 0.1794 | Valid BLEU: 0.0801\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def evaluation(model, dataloader, loss_function, device, index_to_tar):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    total_bleu = 0.0  # 전체 BLEU 점수를 누적할 변수\n",
    "\n",
    "    smooth_fn = SmoothingFunction().method1  # BLEU 점수 계산 시 smoothing method 사용\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n",
    "            encoder_inputs = encoder_inputs.to(device)\n",
    "            decoder_inputs = decoder_inputs.to(device)\n",
    "            decoder_targets = decoder_targets.to(device)\n",
    "\n",
    "            # 순전파\n",
    "            outputs = model(encoder_inputs, decoder_inputs)\n",
    "\n",
    "            # loss 계산\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), decoder_targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산 (padding 제외)\n",
    "            mask = decoder_targets != 0\n",
    "            total_correct += ((outputs.argmax(dim=-1) == decoder_targets) * mask).sum().item()\n",
    "            total_count += mask.sum().item()\n",
    "\n",
    "            # BLEU 점수 계산\n",
    "            batch_size = decoder_targets.size(0)\n",
    "            for i in range(batch_size):\n",
    "                pred_indices = outputs[i].argmax(dim=-1).tolist()  # 예측 토큰 인덱스\n",
    "                target_indices = decoder_targets[i].tolist()       # 정답 토큰 인덱스\n",
    "\n",
    "                # <sos>, <eos>, <pad> 제거\n",
    "                pred_tokens = [index_to_tar[idx] for idx in pred_indices if idx != 0 and idx != 3 and idx != 4]\n",
    "                target_tokens = [index_to_tar[idx] for idx in target_indices if idx != 0 and idx != 3 and idx != 4]\n",
    "\n",
    "                # BLEU 점수 계산\n",
    "                if len(pred_tokens) > 0 and len(target_tokens) > 0:\n",
    "                    bleu = sentence_bleu([target_tokens], pred_tokens, smoothing_function=smooth_fn)\n",
    "                    total_bleu += bleu\n",
    "\n",
    "    # 평균값 계산\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_acc = total_correct / total_count\n",
    "    avg_bleu = total_bleu / len(dataloader.dataset)  # 전체 샘플 수로 나눔\n",
    "\n",
    "    return avg_loss, avg_acc, avg_bleu\n",
    "\n",
    "train_loss, train_acc, train_bleu = evaluation(model, train_dataloader, loss_function, device, index_to_tar)\n",
    "valid_loss, valid_acc, valid_bleu = evaluation(model, valid_dataloader, loss_function, device, index_to_tar)\n",
    "\n",
    "print(f\"Train BLEU: {train_bleu:.4f} | Valid BLEU: {valid_bleu:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
